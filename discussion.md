結論：**そのままじゃ足りない**。極端不均衡の *b* 系で差がつく。下を丸ごと実装して“勝ち筋”を固める。

# 0) 戦略全体像（3段リレー）

**(A) 採掘→(B) 精査→(C) ランク最適化** の三段。

* **(A) 陽性候補採掘（高再現）**：構造規則＋軽量モデルで**候補10–20%**に絞る。
* **(B) 精査（高適合）**：多タスク GNN/トランスフォーマ＋Tabular を**PU＋Focal**で再学習。
* **(C) ランク融合**：fold×器の**順位平均（rank-avg）**＋**正例近傍距離**で再重み付け。

# 1) データ衛生＆リーク遮断（勝敗分岐点）

* **Murcko scaffold group K=10 CV**。同一 scaffold は**必ず同 fold**。InChIKey 1st-block で重複統合。
* **plate/well 補正は train 内の**み：plate, well を要因として**Robust median/IQR** or **ComBat**。
  “問題 plate/well（N8, C1046…）”は**重み0.3–0.7**に下げた版と除外版を両方用意し、後段でアンサンブル。
* **閾値ゆらぎ対策**：境界ラベル（出力値が閾値±ε; 吸収%Tや蛍光強度の元値から構成されているなら）に**ラベルスムージング0.05**。

# 2) 表現（特徴量）層

* **ベース**：ECFP6（2048/4096, count/bit 両方）＋RDKit 2D（TPSA, SlogP, 芳香環・sp²率・二重結合数・最長共役長 proxy・Donor/Acceptor カウント・分子平面性/回転自由度）。
* **GNN 埋め込み**：MPNN/GIN＋Edge features（結合次数/芳香/共役）。**多タスク4ヘッド**（Abs340, Abs450–679, Fluo340/450, FluoAny）。
* **自己教師あり事前学習**：ランダムSMILESマスク＋コントラスト（same-mol augment vs. others）。最低**50–100M step**の軽 pretrain（公開化前提）。
* **物理 proxy（構造から計算）**：

  * **最長共役パス長**、**芳香縮合度**、D–A 指標（陽電性/陰電性置換基の同時存在フラグ）、**塩基性中心–受容中心距離**の統計量。
  * HOMO/LUMO 等の量子記述子は**計算量が重いので不採用**（本戦では“構造起点の規則ベース proxy”で十分差が出る）。

# 3) “候補採掘 (A)”：高再現フィルタ

* **規則**：`Longest_conj_path ≥ L0`、`(Donor≥1 & Acceptor≥1)`、`芳香環数≥2` などで**候補10–20%**。
* **軽量モデル**：ECFP→**ロジスティック回帰（class_weight='balanced'）**で**再現率95%**を目標に粗ふるい。
* **異常検知**（2b 特化）：IsolationForest / One-Class SVM を**陽性周辺の密度推定**として和集合。

# 4) “精査 (B)”：不均衡最適化コア

* **損失**：**Focal(γ=2–3)** + **class weights**。1b/2b は**負例ダウンサンプル1:20〜1:50**（学習毎に再サンプル）。
* **PU 学習**：nnPU を**サブモデル**として学習し、そのスコアを**メタ特徴**に入れる（特に 2b=陽性0.23%）。
* **多タスク転移**：4ヘッドで**shared encoder**、タスク損失は**陽性率の逆数で重み付け**。1a/2a → 1b/2b へ表現を渡す。
* **ハードネガティブ採掘**：各 epoch 後に**誤分類上位の負例**を次 epoch のバッチへ優先投入。

# 5) 学習レジメン

* **CV 設計**：Scaffold-stratified K=10。モデル選択は**PR-AUC**、提出は**ROC-AUC**最良 fold-blend。
* **データ拡張**：ランダムSMILES、原子順序パーミュテーション、GNNのDropEdge。
* **正則化**：Label smoothing 0.05、Dropout 0.2–0.4、Weight decay 1e-5。早期停止は**PR-AUC 目標**で patience 30。

# 6) 器（モデル）構成と比率

* **Tabular 系**：LightGBM（num_leaves 64–256, max_depth 6–12, learning_rate 0.02–0.05, feature_fraction 0.6–0.9, **pos_weight= (N−P)/P**）。
* **DL/GNN 系**：GIN/MPNN（層4–6, hidden 256–512, **Focal**）。
* **蒸留**：GNN の logit→LightGBM に**logit 蒸留特徴**を追加（不均衡下で順位安定化）。
* **二段 MLP**：ECFP+RDKit+PUスコアを入力に**小型 MLP**（ReLU×3, 512→256→64）。

# 7) ランク最適化 (C) とアンサンブル

* **rank-avg**：各 fold×器の**順位を平均**（確率は使わない）。
* **近傍陽性重み**：クエリ化合物の**Tanimoto to nearest positive**を**後段 LambdaMART**の特徴に追加し、最終順位を微調整。
* **キャリブレーション**：CV-out のみ **Isotonic**。提出は確率でも**順位保持**を確認。

# 8) それぞれのタスクでの上積み

* **Abs 1a（340nm）**：データ量が効く。多タスクで**1b と encoder 共有**しつつ、1a 単体でも LGBM×GNN の**50:50**。
* **Abs 1b（450–679, 1.5%）**：**PU＋Focal＋負例1:30**、**ハードネガ採掘**強め。特徴では**最長共役**と**多環縮合度**が鍵。
* **Fluo 2a（340/450）**：**D–A 指標**、ヘテロ原子配置、平面性 proxy が寄与。多タスクで 2b に蒸留。
* **Fluo 2b（any of three, 0.23%）**：

  * **候補採掘を最も強く**（規則∪異常検知で上位20%へ）。
  * **nnPU + Focal**＋**蒸留**＋**ハードネガ**。
  * 2a の上位信頼陽性を**擬似ラベル**として 2b に微弱重み（w=0.2）で転移。

# 9) 安定性・再現性（審査要件クリア）

* 乱数種 10 本 × 10-fold、**ブートストラップ 100 回**で順位分散を監視。
* すべての前処理・記述子算出・学習スクリプトを**Docker/Conda lock**で公開。
* plate/well 補正を**ON/OFF**両方で学習し、**両者をアンサンブル**（過補正リスクのヘッジ）。

# 10) 提出運用（実務）

* 各サブタスク：**Tabular×GNN×PU×（補正ON/OFF）×fold**のrank-avg。
* **早めに固定**：CV→LB で一貫して上位の「安定モデル集合」を決め、**以降は重みだけ最終調整**。
* 監視指標：**CVとLBの順位相関（Spearman）**。これが高い器に重み集中。

---

要点だけまとめると：

* *b* は「**PU＋Focal＋負例強ダウンサンプル＋ハードネガ採掘＋多タスク転移**」が決定打。
* 全体は「**候補採掘 → 精査 → ランク最適化**」の三段構えで**順位安定化**。
* plate/well 補正は**train 内限定**。補正ON/OFFの**両刀アンサンブル**でリスク分散。

これで“勝ちにいく”布陣になる。
