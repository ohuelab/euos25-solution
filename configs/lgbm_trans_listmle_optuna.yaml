# Full Chemeleon+LGBM with Transmittance (quantitative) ListMLE ranking configuration (with Optuna)
# Uses Chemeleon embeddings as features for LightGBM with ListMLE objective

seed: 42
folds: 5
scaffold_min_size: 10

# Task configuration
task: "transmittance340"
metrics:
  - "roc_auc"
  - "pr_auc"

# Featurizers - Chemeleon embeddings
featurizers:
  - name: ecfp
    params:
      radius: 3
      n_bits: 2048
      use_counts: true
  - name: rdkit2d
    params: {}
  - name: conj_proxy
    params:
      L_cut: 4

# Model configuration - LightGBM with ListMLE objective
model:
  name: "lgbm"
  objective_type: "listmle"
  params:
    n_estimators: 1000
    # Parameters below will be tuned by Optuna
    learning_rate: 0.02
    max_depth: -1
    num_leaves: 255
    subsample: 0.8
    colsample_bytree: 0.8
    min_child_samples: 10
    reg_alpha: 0.1
    reg_lambda: 0.1

# Imbalance handling - Use quantitative values
imbalance:
  use_pos_weight: false
  use_focal_loss: false
  use_quantitative: true
  quantitative_normalize: true

# Early stopping
early_stopping_rounds: 200
early_stopping_metric: "roc_auc"

# Logging
log_level: "INFO"

# Optuna hyperparameter tuning configuration
optuna:
  enable: true
  n_trials: 100
  timeout: null  # No timeout (in seconds)
  study_name: chemeleon_lgbm_listmle_tuning

  # LGBM parameters to tune
  # Any parameter not listed here will use the fixed value from model.params
  lgbm_params:
    learning_rate:
      type: float
      min: 0.01
      max: 0.08
    num_leaves:
      type: int
      min: 63
      max: 511
    min_child_samples:
      type: int
      min: 5
      max: 50
    reg_alpha:
      type: float
      min: 0.001
      max: 1.0
      log: true
  feature_groups:
    tune: false

# Note: Chemeleon foundation model weights (chemeleon_mp.pt) must be downloaded separately
# See: https://chemprop.readthedocs.io/en/latest/chemeleon_foundation_finetuning.html

