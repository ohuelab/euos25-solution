# Full Chemeleon with Focal Loss configuration
# Uses Chemeleon foundation model with Focal Loss for imbalanced data

seed: 42
folds: 5
scaffold_min_size: 10

# Task configuration
task: "transmittance340"
metrics:
  - "roc_auc"
  - "pr_auc"

# Model configuration
model:
  name: "chemprop"
  params:
    max_epochs: 1000
    batch_size: 128
    learning_rate: 0.0001  # Lower LR for foundation model fine-tuning
    hidden_size: 300
    depth: 3
    dropout: 0.1
    aggregation: "mean"
    batch_norm: false  # Typically false with foundation models
    ffn_num_layers: 2
    use_foundation: true
    foundation_name: "chemeleon"
    use_focal_loss: true  # Enable Focal Loss
    focal_alpha: 0.25  # Weighting factor for positive class
    focal_gamma: 2.0  # Focusing parameter
    checkpoint_dir: "outputs/full_chemeleon_focal/checkpoints"

# No featurizers needed for ChemProp
featurizers: []

# Imbalance handling (focal loss handles this)
imbalance:
  use_pos_weight: false
  pos_weight_from_data: false
  use_sampling: false
  use_focal_loss: false

# Optuna (can be enabled for hyperparameter tuning)
optuna:
  enable: false

# Early stopping
early_stopping_rounds: 200
early_stopping_metric: "roc_auc"

# Logging
log_level: "INFO"

# Note: Chemeleon foundation model weights (chemeleon_mp.pt) must be downloaded separately
# See: https://chemprop.readthedocs.io/en/latest/chemeleon_foundation_finetuning.html
