# Full Chemeleon+LGBM with Focal Loss configuration
# Uses Chemeleon embeddings as features for LightGBM with Focal Loss for imbalanced data

seed: 42
folds: 5
scaffold_min_size: 10

# Featurizers - Chemeleon embeddings

featurizers:
  - name: ecfp
    params:
      radius: 3
      n_bits: 2048
      use_counts: true
  - name: rdkit2d
    params: {}
  - name: mordred
    params:
      ignore_3d: true
  - name: chemeleon
    params:
      chemeleon_path: "data/pretrained/chemeleon_mp.pt"
      aggregation: "mean"
      device: null  # Auto-detect (cuda/mps/cpu)
  - name: conj_proxy
    params:
      L_cut: 4
  - name: chemberta
    params:
      pooling: mean

# Model configuration - LightGBM
model:
  name: "lgbm"
  params:
    n_estimators: 1000
    # Parameters below will be tuned by Optuna
    learning_rate: 0.02
    max_depth: -1
    num_leaves: 255
    subsample: 0.8
    colsample_bytree: 0.8
    min_child_samples: 10
    reg_alpha: 0.1
    reg_lambda: 0.1

# Imbalance handling - Focal Loss
imbalance:
  use_pos_weight: false
  use_focal_loss: true  # Set to false and use_pos_weight: true to use pos_weight instead
  # Focal loss parameters will be tuned by Optuna
  focal_alpha: 0.25
  focal_gamma: 2.0
  # Alternative: use pos_weight instead
  # use_pos_weight: true
  # pos_weight_from_data: true
  # use_focal_loss: false

plates:
  normalize: false

task: y_fluo_any
metrics:
  - roc_auc
  - pr_auc

early_stopping_rounds: 200
early_stopping_metric: roc_auc

log_level: INFO

# Optuna hyperparameter tuning configuration
optuna:
  enable: true
  n_trials: 100
  timeout: null  # No timeout (in seconds)
  study_name: chemeleon_lgbm_focal_tuning

  # LGBM parameters to tune
  # Any parameter not listed here will use the fixed value from model.params
  lgbm_params:
    learning_rate:
      type: float
      min: 0.01
      max: 0.08
    num_leaves:
      type: int
      min: 63
      max: 511
    min_child_samples:
      type: int
      min: 5
      max: 50
    reg_alpha:
      type: float
      min: 0.001
      max: 1.0
      log: true
  # Focal loss parameters to tune (when use_focal_loss=True)
  focal_params:
    focal_alpha:
      type: float
      min: 0.1
      max: 0.5
    focal_gamma:
      type: float
      min: 1.0
      max: 3.0
    focal_scale:
      type: float
      min: 10.0
      max: 200.0

# Note: Chemeleon foundation model weights (chemeleon_mp.pt) must be downloaded separately
# See: https://chemprop.readthedocs.io/en/latest/chemeleon_foundation_finetuning.html
