# Full Chemeleon+LGBM configuration
# Uses Chemeleon embeddings as features for LightGBM

seed: 42
folds: 5
scaffold_min_size: 10

# Task configuration
task: "transmittance340"
metrics:
  - "roc_auc"
  - "pr_auc"

# Featurizers - Chemeleon embeddings
featurizers:
  - name: "chemeleon"
    params:
      chemeleon_path: "chemeleon_mp.pt"
      aggregation: "mean"
      device: null  # Auto-detect (cuda/mps/cpu)

# Model configuration - LightGBM
model:
  name: "lgbm"
  params:
    boosting_type: "gbdt"
    objective: "binary"
    metric: "auc"
    learning_rate: 0.05
    num_leaves: 63
    max_depth: -1
    min_child_samples: 20
    subsample: 0.8
    subsample_freq: 1
    colsample_bytree: 0.8
    reg_alpha: 0.1
    reg_lambda: 0.1
    n_estimators: 1000
    random_state: 42
    n_jobs: -1
    verbose: -1

# Imbalance handling
imbalance:
  use_pos_weight: true
  pos_weight_from_data: true
  use_sampling: false
  use_focal_loss: false

# Optuna (can be enabled for hyperparameter tuning)
optuna:
  enable: false
  n_trials: 100
  timeout: null
  study_name: "chemeleon_lgbm_optuna"
  storage_enable: true
  lgbm_params:
    learning_rate:
      type: "float"
      min: 0.01
      max: 0.1
      log: true
    num_leaves:
      type: "int"
      min: 31
      max: 127
    max_depth:
      type: "int"
      min: 3
      max: 12

# Early stopping
early_stopping_rounds: 50
early_stopping_metric: "roc_auc"

# Logging
log_level: "INFO"

# Note: Chemeleon foundation model weights (chemeleon_mp.pt) must be downloaded separately
# See: https://chemprop.readthedocs.io/en/latest/chemeleon_foundation_finetuning.html
