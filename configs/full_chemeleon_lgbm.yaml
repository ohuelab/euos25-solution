# Full Chemeleon+LGBM configuration
# Uses Chemeleon embeddings as features for LightGBM

seed: 42
folds: 5
scaffold_min_size: 10

# Task configuration
task: "transmittance340"
metrics:
  - "roc_auc"
  - "pr_auc"

# Featurizers - Chemeleon embeddings
featurizers:
  - name: "chemeleon"
    params:
      chemeleon_path: "data/pretrained/chemeleon_mp.pt"
      aggregation: "mean"
      device: null  # Auto-detect (cuda/mps/cpu)

# Model configuration - LightGBM
model:
  name: "lgbm"
  params:
    n_estimators: 1000
    # Parameters below will be tuned by Optuna
    learning_rate: 0.02
    max_depth: -1
    num_leaves: 255
    subsample: 0.8
    colsample_bytree: 0.8
    min_child_samples: 10
    reg_alpha: 0.1
    reg_lambda: 0.1

# Imbalance handling
imbalance:
  use_pos_weight: true
  pos_weight_from_data: true
  use_sampling: false
  use_focal_loss: false

# Early stopping
early_stopping_rounds: 200
early_stopping_metric: "roc_auc"

# Logging
log_level: "INFO"

# Optuna hyperparameter tuning configuration
optuna:
  enable: true
  n_trials: 100
  timeout: null  # No timeout (in seconds)
  study_name: chemeleon_lgbm_tuning

  # LGBM parameters to tune
  # Any parameter not listed here will use the fixed value from model.params
  lgbm_params:
    learning_rate:
      type: float
      min: 0.01
      max: 0.08
    num_leaves:
      type: int
      min: 63
      max: 511
    min_child_samples:
      type: int
      min: 5
      max: 50
    reg_alpha:
      type: float
      min: 0.001
      max: 1.0
      log: true
  pos_weight_params:
    pos_weight_multiplier:
      type: float
      min: 0.5
      max: 2.0
  feature_groups:
    tune: false

# Note: Chemeleon foundation model weights (chemeleon_mp.pt) must be downloaded separately
# See: https://chemprop.readthedocs.io/en/latest/chemeleon_foundation_finetuning.html
