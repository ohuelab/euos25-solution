# Full Chemeleon configuration for production training
# Uses ChemProp with Chemeleon foundation model

seed: 42
folds: 5
scaffold_min_size: 10

# Task configuration
task: "transmittance340"
metrics:
  - "roc_auc"
  - "pr_auc"

# Model configuration
model:
  name: "chemprop"
  params:
    max_epochs: 50
    batch_size: 64
    learning_rate: 0.0001  # Lower LR for foundation model fine-tuning
    hidden_size: 300
    depth: 3
    dropout: 0.1
    aggregation: "mean"
    batch_norm: false  # Typically false with foundation models
    ffn_num_layers: 2
    use_foundation: true
    foundation_name: "chemeleon"
    checkpoint_dir: "outputs/full_chemeleon/checkpoints"

# No featurizers needed for ChemProp
featurizers: []

# Imbalance handling
imbalance:
  use_pos_weight: false
  pos_weight_from_data: false
  use_sampling: false
  use_focal_loss: false

# Optuna (can be enabled for hyperparameter tuning)
optuna:
  enable: false

# Early stopping
early_stopping_rounds: 10
early_stopping_metric: "roc_auc"

# Logging
log_level: "INFO"

# Note: Chemeleon foundation model weights (chemeleon_mp.pt) must be downloaded separately
# See: https://chemprop.readthedocs.io/en/latest/chemeleon_foundation_finetuning.html
